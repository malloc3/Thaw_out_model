#TODO
# 1. Make it save progress more effectively.   And write a different script to compllete analysis
#     This line does all the heavy lifting and should ideally only be run once for each condition.
#    The save needs to record the "melt-out-date" and Melt out date calculation params used for each location and each year.
#     swes_melt_out_dates = swes_df %>%
#        group_by(site_id, melt_types) %>%
#        mutate(is_melt_out_date = vectorized_is_melt_date_sum(Date, .data, swes_df, all_or_any_string = "all", Snot_melt = 5, previous_days = 5))
#
# 2. Needs to better handle multiple data CSV's with multiple years.   Its unreasonable to require all the years at all locations be included in the same
#    dataframe.   That takes way to long to run for all the scripts and this one too.  So chunking years makes sense.   There should be an easy way to record that info
#
# 3. Make a progress bar!  That couldbe helpful
# 4. Multi thread this process.   

library(RPostgres) # The database driver
library(DBI) # Functions needed to interact with the database
library(rstudioapi) # Package that asked for credentials
library(tcltk)
library(rstudioapi)
library(lubridate)
library(dplyr)
library(ggplot2)
#------------- Add functions ---------------------------------------------------#
source("../Pull_Ribbitr_Ponds_Snow_melt/f_connect_to_database.R")
source("f_matlab_to_datetime.R")

# --------------------- Install Required Packages ----------------------------#
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}
# librarian downloads, if not already downloaded, and reads in needed packages
librarian::shelf(tidyverse, DBI, RPostgres, dbplyr, kableExtra, tcltk)
# ------------------------------------------------------------------------------#


#  User Designated variables
debug = FALSE
specific_melt_df_file = FALSE #Can set a specific file if you want.  Otherwise will only get the most recent files
# This file must have been generated by the Parse_SPIReS_MODIS scripts. and be in the right DF format
# Column Names:  (probably... unless something changed)
# ["site_id", Date", "Sum Melt", "Mean Melt", "Median Melt", "Max Melt", "Min Melt", "Standard Deviation Melt", "Variance Melt", "Region_Size_m", "Region_Shape", others?]


#--------------------------- End User Designated variables----------------------#






#--------------------------- Get Secrest and File Paths for logs----------------#
# This may need to be updated for your computer.  But this should find file in the master directory for this git project
#. Make sure to update the UPDATE_local_directories.csv file with the correct directories
csv_with_directory_info = read.csv(paste(getwd(), "/../", "UPDATE_local_directories.csv", sep = ""))
secrets_file = csv_with_directory_info[1,2] # Gets user secrets file path
SPIRES_log_directory = csv_with_directory_info[3,2] #Gets the SPIRES logs directory file path (debug folder should be here too)





#--------------------------- Handles Debug Conditions---------------------------#
if (debug){
  snow_log_save_path = paste(SPIRES_log_directory, "/debug", sep = "")
}else{  # If not debug
  snow_log_save_path = paste(SPIRES_log_directory, "/logs", sep = "")
}





#--------------------------- Find proper melt_df file path-----------------------#
#Get the most recently created CSV file in the snow_log_save_path directory. (either debug or normal)
# Defaults to user specified melt directory (if applicable)
if (specific_melt_df_file == FALSE){ # The user did NOT specify a specific file path
  files <- list.files(snow_log_save_path, full.names = TRUE)
  
  # Filter only CSV files
  csv_files <- files[grep("\\.csv$", files)]
  
  # Get file info
  file_info <- file.info(csv_files)
  
  # Find the most recently created file
  snow_df_filepath <- csv_files[which.max(file_info$ctime)]
}else{ #User specified a specific file path
  snow_df_filepath = specific_melt_file #set to user file.
}


# Read the CSV file
snow_df <- read.csv(snow_df_filepath)

# ------------------- Drop nans ------------------------------------------------#
# Realistically there shouldnt be any nans... But it appears there are.  This
# Is probably because the snow melt data is not calculated everywhere perfectly
# but I am unsure.   This doesn't affect this protocol that much but it may
# change the parse_SPIRES_Modis values...  To be thought through carefully
before = nrow(snow_df)
snow_df = na.omit(snow_df)
after = nrow(snow_df)
diff = before - after
if (diff > 0){
  print("There were NAN rows omitted")
  print(diff)
}


# ------------------ Convert all matlab dats to R dates ------------------------#
# We now have all our melt data.  And we want to figure out "melt out" date for each pond and each year.
#  Lets define "melt out" as the date where AFTER BEING HIGH SNOW for some amount of time the SWE or SWE Hybrid decreases below some threshold.
snow_df = snow_df %>%
  mutate(Date = matlab2date(Date))

#------------------ Now lest sort by melt_types ------------- ------------------#
melt_df = snow_df[snow_df$melt_types == "melt", ]
swes_df = snow_df[snow_df$melt_types != "melt", ] # A dataframe with only the swe and SweHybrid measurements
                    

#---------- Now lets get a DF of all the Melt out Dates ------------------------#
# Problematically the pipe format isn't quite working with my functions.  I think
# This is because it is passing the Date as a list istead of an individual Date objectf

# sweHybrid_df = snow_df[snow_df$melt_types == "sweHybrid", ]
#sweHybrid_melt_out_dates = sweHybrid_df %>%
#  group_by(site_id) %>%
#  mutate(is_melt_out_date = vectorized_is_melt_date_sum(Date, .data, sweHybrid_df, all_or_any_string = "all", Snot_melt = 10))

# swe_df = snow_df[snow_df$melt_types == "swe", ]
#swe_melt_out_dates = swe_df %>%
#  group_by(site_id) %>%
#  mutate(is_melt_out_date = vectorized_is_melt_date_sum(Date, .data, swe_df, all_or_any_string = "all", Snot_melt = 10))



# This finds the melt dates based on SWE and SWEHybrid values all at once
# May want to break this apart so SWE and SWEHybrid can have different params
swes_melt_out_dates = swes_df %>%
  group_by(site_id, melt_types) %>%
  mutate(is_melt_out_date = vectorized_is_melt_date_sum(Date, .data, swes_df, all_or_any_string = "all", Snot_melt = 5, previous_days = 5))


# This will plot our melt outs.   It plots all of the sites in one figure which
#    is not conducive for large datasets...   But can be conducive for debugging
if(debug){
  ggplot(data = swes_melt_out_dates) +
    geom_line(mapping = aes(x = Date, y = Sum.Melt, color = melt_types, linetype = site_id))+
    geom_vline(data = swes_melt_out_dates[swes_melt_out_dates$is_melt_out_date, ], aes(xintercept = Date, color = melt_types, linetype = site_id), linewidth=.75) +
    scale_x_date(date_labels = "%Y-%m-%d")
}








#--------------------Melt out VS DATA aquired-----------------------------------#

# We now have all the dates when things melt out.   The next step is to find out
# the dates of all the samples we have taken!

# First lets get all the site_ids that we have melt dates for
unique_site_ids = unique(swes_melt_out_dates$site_id)


#----- Lets connect to the DB

# --------------------- Establish Connection to Ribbitr Database --------------#
ribbitr_connection = connect_to_database(secrets_file)

# setting your search path
dbExecute(conn = ribbitr_connection,
          statement = "set search_path = 'survey_data'")

# --------Fetch data from ribbitr database and combine into local DF ----------#
db_data <- tbl(ribbitr_connection, "location") %>%
  inner_join(tbl(ribbitr_connection, "region"), by = c("location_id")) %>%
  inner_join(tbl(ribbitr_connection, "site"), by = c("region_id")) %>% 
  inner_join(tbl(ribbitr_connection, "visit"), by = c("site_id")) %>%
  inner_join(tbl(ribbitr_connection, "survey"), by = c("visit_id")) %>%
  inner_join(tbl(ribbitr_connection, "capture"), by = c("survey_id")) %>%
  inner_join(tbl(ribbitr_connection, "qpcr_bd_results"), by = c("bd_swab_id")) %>%
  filter(site_id %in% unique_site_ids)
# ------------------------------------------------------------------------------#

print("Reformatting Data (this may take some time)")
clean_data <- db_data %>%
  collect()


# ----- Now lets get ONLY melt out dates ---------------------------------------#

#Rename some column names (probably should just make everythign lower case...)
swes_only_melt_out_dates = swes_melt_out_dates[swes_melt_out_dates$is_melt_out_date, ]
swes_only_melt_out_dates = rename(swes_only_melt_out_dates, year = Year)
swes_only_melt_out_dates = rename(swes_only_melt_out_dates, melt_out_date = Date)
swes_only_melt_out_dates = swes_only_melt_out_dates %>%
  mutate(date = ymd(melt_out_date))



clean_data_year = clean_data %>%
  mutate(date = ymd(date)) %>%
  mutate(year = year(date))
clean_data_year = rename(clean_data_year, measurement_date = date)

# So we now have all the data that corrosponds with years that we have valid melt outs in
clean_data_year = clean_data_year[clean_data_year$year %in% unique(swes_only_melt_out_dates$year), ]


# WARNING This seems to be working right.   The determination of "MELT OUT DATE" seems off.  This could be caused by sites never actually getting snow...  
# Next we join the dataframes, find the difference and then select only the years
# that have a positive difference but that are also less than 365 days
joined_df <- inner_join(clean_data_year, swes_only_melt_out_dates, by = c("year"), relationship = "many-to-many")

# Calculate the time difference in days between the two dates
joined_df <- joined_df %>%
  mutate(days_difference = measurement_date - melt_out_date)

#Filter to find only the positive difference less than 365 days
positive_diff <- joined_df %>%
  filter(days_difference > 0) %>%
  filter(days_difference < 365)


ggplot(positive_diff) +
  geom_histogram(aes(x =  days_difference), binwidth = 1, fill = "skyblue", color = "black")+
  xlim(c(0, 365)) +
  labs(title = "Histogram of Known Measurement Days after Meltout",
       x = "Number of Days",
       y = "Count")
f_subset <- joined_df[, c("measurement_date", "melt_out_date", "days_difference")]


